import os
import io
import requests
import tempfile
from typing import List
from docx import Document
from docx.shared import Inches
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import logging
from bs4 import BeautifulSoup
import re

logger = logging.getLogger(__name__)

class GoogleDocsMerger:
    def __init__(self, credentials_file: str):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Google Docs
        
        Args:
            credentials_file: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —É—á–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö Google Service Account
        """
        self.credentials_file = credentials_file
        self.client = None
        self.docs_service = None
        self.drive_service = None
        self._connect()
    
    def _connect(self):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Google Docs API"""
        try:
            scope = [
                'https://www.googleapis.com/auth/documents',
                'https://www.googleapis.com/auth/drive',
                'https://spreadsheets.google.com/feeds',
                'https://www.googleapis.com/auth/drive.readonly'
            ]
            creds = ServiceAccountCredentials.from_json_keyfile_name(
                self.credentials_file, scope)
            
            # –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è gspread
            self.client = gspread.authorize(creds)
            
            # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è Google Docs API
            self.docs_service = build('docs', 'v1', credentials=creds)
            self.drive_service = build('drive', 'v3', credentials=creds)
            
            logger.info("–£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Google APIs")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Google APIs: {e}")
            return False

    def check_document_access(self, doc_url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É"""
        try:
            doc_id = self.extract_doc_id_from_url(doc_url)
            if not doc_id:
                return False
            
            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document = self.docs_service.documents().get(documentId=doc_id).execute()
            logger.info(f"–î–æ—Å—Ç—É–ø –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É –µ—Å—Ç—å: {document.get('title', 'Unknown')}")
            return True
        except HttpError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞: {e}")
            return False
    
    def extract_doc_id_from_url(self, url: str) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ URL Google Docs
        """
        try:
            if '/document/d/' in url:
                doc_id = url.split('/document/d/')[1].split('/')[0]
            elif 'id=' in url:
                doc_id = url.split('id=')[1].split('&')[0]
            else:
                doc_id = url.split('/')[-1].split('?')[0]
            
            return doc_id
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è ID –∏–∑ URL {url}: {e}")
            return None
    
    def get_document_content_with_images(self, doc_url: str) -> dict:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ Google Doc —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        """
        try:
            doc_id = self.extract_doc_id_from_url(doc_url)
            if not doc_id:
                return {'text': '', 'images': []}
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ HTML —ç–∫—Å–ø–æ—Ä—Ç
            logger.info("–ü—Ä–æ–±—É–µ–º –º–µ—Ç–æ–¥ HTML —ç–∫—Å–ø–æ—Ä—Ç–∞...")
            result = self.get_document_content_with_images_improved(doc_url)
            
            if result['images']:
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(result['images'])} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ HTML —ç–∫—Å–ø–æ—Ä—Ç")
                return result
            
            # –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ—Ç, –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ Drive API
            logger.info("–ü—Ä–æ–±—É–µ–º –º–µ—Ç–æ–¥ Drive API...")
            images_via_drive = self.get_images_via_drive_api(doc_id)
            if images_via_drive:
                text_content = self._get_document_content_simple(doc_url)
                return {'text': text_content, 'images': images_via_drive}
            
            # –ï—Å–ª–∏ –≤—Å–µ –º–µ—Ç–æ–¥—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
            logger.info("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç")
            text_content = self._get_document_content_simple(doc_url)
            return {'text': text_content, 'images': []}
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return {'text': '', 'images': []}

    def get_document_content_with_images_improved(self, doc_url: str) -> dict:
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ —ç–∫—Å–ø–æ—Ä—Ç HTML
        """
        try:
            doc_id = self.extract_doc_id_from_url(doc_url)
            if not doc_id:
                return {'text': '', 'images': []}

            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ HTML
            export_url = f"https://docs.google.com/document/d/{doc_id}/export?format=html"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(export_url, headers=headers, timeout=30)
            
            if response.status_code != 200:
                logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ HTML: {response.status_code}")
                return {'text': '', 'images': []}

            html_content = response.text
            text_content, images_data = self._parse_html_content(html_content, doc_id)
            
            return {'text': text_content, 'images': images_data}
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ: {e}")
            return {'text': '', 'images': []}

    def _parse_html_content(self, html_content: str, doc_id: str):
        """–ü–∞—Ä—Å–∏—Ç HTML –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Ç–µ–≥–∏
            for element in soup(['script', 'style', 'meta', 'link']):
                element.decompose()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text_content = soup.get_text(separator='\n', strip=True)
            text_content = re.sub(r'\n\s*\n', '\n\n', text_content)  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images_data = []
            img_tags = soup.find_all('img')
            
            for i, img_tag in enumerate(img_tags):
                try:
                    img_src = img_tag.get('src', '')
                    if not img_src:
                        continue
                        
                    # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    image_data = self._download_image_from_url(img_src)
                    if image_data:
                        images_data.append({
                            'data': image_data,
                            'index': i,
                            'alt': img_tag.get('alt', f'Image {i+1}'),
                            'width': img_tag.get('width', '400'),
                            'height': img_tag.get('height', '300')
                        })
                        logger.info(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ")
                        
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i}: {e}")
                    continue
            
            return text_content, images_data
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML: {e}")
            return "", []

    def _download_image_from_url(self, image_url: str) -> bytes:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ URL"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'image/webp,image/apng,image/*,*/*',
                'Accept-Language': 'en-US,en;q=0.9',
            }
            
            response = requests.get(image_url, headers=headers, timeout=30, stream=True)
            if response.status_code == 200:
                return response.content
            else:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {response.status_code}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        return None

    def get_images_via_drive_api(self, doc_id: str):
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ Drive API"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            results = self.drive_service.files().list(
                q=f"'{doc_id}' in parents and mimeType contains 'image/'",
                fields="files(id, name, mimeType, size)"
            ).execute()
            
            images = results.get('files', [])
            images_data = []
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Drive API")
            
            for img in images:
                try:
                    # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    request = self.drive_service.files().get_media(fileId=img['id'])
                    image_content = request.execute()
                    
                    images_data.append({
                        'data': image_content,
                        'name': img['name'],
                        'mime_type': img['mimeType'],
                        'size': img.get('size', 0)
                    })
                    
                    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {img['name']}")
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img['name']}: {e}")
                    continue
                    
            return images_data
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Drive API: {e}")
            return []

    def _get_document_content_simple(self, doc_url: str) -> str:
        """
        –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —á–µ—Ä–µ–∑ —ç–∫—Å–ø–æ—Ä—Ç
        """
        try:
            doc_id = self.extract_doc_id_from_url(doc_url)
            if not doc_id:
                return ""
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∞
            formats = ['txt', 'html']
            
            for format_type in formats:
                try:
                    export_url = f"https://docs.google.com/document/d/{doc_id}/export?format={format_type}"
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    }
                    response = requests.get(export_url, headers=headers, timeout=30)
                    
                    if response.status_code == 200:
                        if format_type == 'html':
                            # –î–ª—è HTML –∏–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
                            return self._extract_text_from_html(response.text)
                        else:
                            return response.text
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ {format_type}: {e}")
                    continue
            
            return ""
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ: {e}")
            return ""

    def _extract_text_from_html(self, html_content):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ HTML"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Ç–µ–≥–∏
            for element in soup(['script', 'style', 'meta', 'link']):
                element.decompose()
            
            # –ü–æ–ª—É—á–∞–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
            text = soup.get_text(separator='\n', strip=True)
            text = re.sub(r'\n\s*\n', '\n\n', text)  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã
            
            return text
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫
            clean = re.compile('<.*?>')
            return re.sub(clean, '', html_content)
    
    def merge_documents_with_images(self, doc_urls: List[str], output_path: str) -> bool:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ Google Docs –≤ –æ–¥–∏–Ω Word-–¥–æ–∫—É–º–µ–Ω—Ç —Å –ø–æ–ø—ã—Ç–∫–æ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        try:
            doc = Document()
            title = doc.add_heading('–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç', 0)
            doc.add_paragraph(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(doc_urls)}")
            doc.add_paragraph()
            
            total_images = 0
            
            for i, doc_url in enumerate(doc_urls, 1):
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {i}/{len(doc_urls)}: {doc_url}")
                
                if i > 1:
                    doc.add_paragraph("\n" + "="*50 + "\n")
                
                doc.add_heading(f'–î–æ–∫—É–º–µ–Ω—Ç {i}', level=1)
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
                content_data = self.get_document_content_with_images(doc_url)
                text_content = content_data['text']
                images = content_data['images']
                
                total_images += len(images)
                logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç {i}: {len(text_content)} —Å–∏–º–≤–æ–ª–æ–≤, {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                
                if text_content:
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    paragraphs = text_content.split('\n')
                    
                    for paragraph in paragraphs:
                        if paragraph.strip():
                            doc.add_paragraph(paragraph.strip())
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –∫–æ–Ω–µ—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    if images:
                        doc.add_paragraph("\n" + "-" * 30 + " –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø " + "-" * 30 + "\n")
                        
                        for img_idx, image_info in enumerate(images, 1):
                            try:
                                image_data = image_info['data']
                                if image_data:
                                    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                    image_stream = io.BytesIO(image_data)
                                    
                                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
                                    doc.add_paragraph(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_idx}: {image_info.get('alt', '')}")
                                    
                                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                                    doc.add_picture(image_stream, width=Inches(5.0))
                                    
                                    # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                    doc.add_paragraph()
                                    
                                    logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_idx} –≤ –¥–æ–∫—É–º–µ–Ω—Ç {i}")
                                    
                            except Exception as e:
                                logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_idx}: {e}")
                                doc.add_paragraph(f"[–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_idx}]")
                else:
                    doc.add_paragraph(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {i}")
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç {i}")
            
            doc.save(output_path)
            logger.info(f"–£—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: {output_path}")
            logger.info(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_images}")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return False

    # –°—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    def merge_documents(self, doc_urls: List[str], output_path: str) -> bool:
        return self.merge_documents_with_images(doc_urls, output_path)


# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥: —ç–∫—Å–ø–æ—Ä—Ç –≤ PDF –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è
def export_as_pdf_and_convert(doc_url: str, output_path: str) -> bool:
    """
    –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥: —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∫–∞–∫ PDF –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ Word
    –≠—Ç–æ –º–æ–∂–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
    """
    try:
        import requests
        from pdf2docx import Converter
        
        doc_id = doc_url.split('/document/d/')[1].split('/')[0]
        pdf_url = f"https://docs.google.com/document/d/{doc_id}/export?format=pdf"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(pdf_url, headers=headers, timeout=30)
        if response.status_code == 200:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π PDF
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_pdf:
                temp_pdf.write(response.content)
                temp_pdf_path = temp_pdf.name
            
            try:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF –≤ Word
                cv = Converter(temp_pdf_path)
                cv.convert(output_path)
                cv.close()
                
                logger.info(f"–£—Å–ø–µ—à–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ PDF: {output_path}")
                return True
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                os.unlink(temp_pdf_path)
        
        logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ PDF: {response.status_code}")
        return False
        
    except ImportError:
        logger.error("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ pdf2docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pdf2docx")
        return False
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ —á–µ—Ä–µ–∑ PDF: {e}")
        return False


# –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–∞
def check_document_permissions(merger: GoogleDocsMerger, doc_urls: List[str]):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø –∫–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
    for i, doc_url in enumerate(doc_urls, 1):
        logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É {i}: {doc_url}")
        has_access = merger.check_document_access(doc_url)
        
        if has_access:
            logger.info(f"‚úÖ –î–æ—Å—Ç—É–ø –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É {i} –ï–°–¢–¨")
        else:
            logger.warning(f"‚ùå –î–æ—Å—Ç—É–ø–∞ –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É {i} –ù–ï–¢")
            
            # –î–∞–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –¥–æ—Å—Ç—É–ø–∞
            doc_id = merger.extract_doc_id_from_url(doc_url)
            if doc_id:
                logger.info(f"üí° –î–ª—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–∞:")
                logger.info(f"1. –û—Ç–∫—Ä–æ–π—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–æ—Å—Ç—É–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
                logger.info(f"2. –î–æ–±–∞–≤—å—Ç–µ email —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ –∫–∞–∫ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞")
                logger.info(f"3. –ò–ª–∏ –æ—Ç–∫—Ä–æ–π—Ç–µ –¥–æ—Å—Ç—É–ø '–í—Å–µ, —É –∫–æ–≥–æ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∞'")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
# def main():
#     merger = GoogleDocsMerger('credentials.json')
    
#     doc_urls = [
#         'https://docs.google.com/document/d/1009vY3SSX_BUyU8wGD9YQrFETRB5o34FNBmgOA-VAwk/edit?usp=sharing',
#     ]
    
#     # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø
#     check_document_permissions(merger, doc_urls)
    
#     # –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
#     success = merger.merge_documents_with_images(doc_urls, '–æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π_–¥–æ–∫—É–º–µ–Ω—Ç.docx')
    
#     if not success:
#         # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ PDF
#         print("–ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ PDF...")
#         success = export_as_pdf_and_convert(doc_urls[0], '–æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π_–¥–æ–∫—É–º–µ–Ω—Ç_–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π.docx')
    
#     if success:
#         print("‚úÖ –î–æ–∫—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã!")
#     else:
#         print("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

# if __name__ == "__main__":
#     main()